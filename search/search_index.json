{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"HPC@Azure","text":"<p>This repo contains Infrastructure-as-a-Code (IaaC) for deploying an HPC cluster on Azure.</p> <p>The code has been developed in order to support the pre-operational phase of the Seareport Project.  We\u2019ve extracted the core of the codebase and made it available in this repository, hoping it might serve others as well.</p> <p>During this process, we\u2019ve simplified several aspects and removed implementation specific details. For instance, Seareport deploys Thalassa, a web application designed for visualizing large scale sea level data on unstructured mesh data. However, this specific application may not be useful for other projects, so we\u2019ve replaced this component with a placeholder, in this case, an empty Virtual Machine.</p> <p>In the same spirit, the actual code that provisions the other VMs and the HPC cluster has been removed.  What this repository offers is a straightforward way to replicate Seareport\u2019s high-level design. To extract value from it, you\u2019ll need to tailor it to your specific needs.</p>"},{"location":"index.html#objectives","title":"Objectives","text":"<p>The main objectives include:</p> <ul> <li>Facilitating the effortless deployment of two separate HPC instances that can be dynamically scaled according to demand.</li> <li>Utilizing the first HPC instance for the execution of a model in an \u201coperational\u201d mode      (for instance, running the model bi-daily) and for the storage and distribution of the outcomes.</li> <li>Employing the second HPC instance to aid in the ongoing refinement of the model.</li> </ul> <p>It is crucial that these two HPC instances operate independently of each other. Any \u201cdevelopment\u201d tasks should be conducted in a manner that does not disrupt or interfere with the operational setup.</p> <p>To achieve this goal a Bicep-based IaaC (Infrastructure-as-a-Code) solution has been developed.</p>"},{"location":"index.html#repository-directory-structure","title":"Repository directory structure","text":"<p>There are three main directories:</p> <ul> <li>The <code>infra</code> directory contains the bicep modules that manage the creation of resources on Azure</li> <li>The <code>provisioning</code> directory contains code helpers that can be used to provision the VMs using <code>ansible</code>.    This directory only contains sample playbooks.   The actual provisioning is omitted since it is application/project specific.</li> <li>The <code>docs</code> directory contains the code that generates the current document.</li> </ul>"},{"location":"index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure cli</li> <li>Azure bicep</li> <li>ansible for provisioning (optional)</li> <li>python 3 for building docs</li> </ul>"},{"location":"docs.html","title":"Documentation","text":""},{"location":"docs.html#initial-setup","title":"Initial Setup","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"docs.html#serve-docs-locally","title":"Serve docs locally","text":"<p>In order to build and serve the docs locally you should run:</p> <pre><code>make docs\n</code></pre>"},{"location":"docs.html#export-to-pdf","title":"Export to pdf","text":"<ol> <li>Serve the docs locally </li> <li>visit this link</li> <li>Print the web page (i.e. press <code>Ctrl + P</code>).</li> </ol>"},{"location":"infra.html","title":"Infrastructure","text":""},{"location":"infra.html#quick-start","title":"Quick start","text":"<ol> <li>Determine the Azure Subscription ID and the correct Tenant ID for your subscription using these     instructions</li> <li>Login to Entra ID with azure-cli. The tenant UUID is the one you determined in step 1. E.g.: <pre><code>az login --tenant 1647eb98-698c-4714-b3b5-8cc15f794d36\n</code></pre></li> <li>Ensure that everything worked as expected: <pre><code>az account show\n</code></pre></li> <li>Enter the <code>infra</code> directory: <pre><code>cd infra\n</code></pre></li> <li> <p>Edit the <code>main.bicepparam</code> file and fill in appropriate values to it.    At the very least you should change <code>project</code> and the SSH public key value.    You can identify the principal IDs by using a command like this:    <pre><code> az ad user show --id 'George.Breyiannis.ec.europa.eu#EXT#@europeancommissionhotmai379.onmicrosoft.com'\n</code></pre></p> </li> <li> <p>Deploy the infrastructure with the following command (it requires confirmation): <pre><code>az deployment sub create -c --template-file main.bicep --parameters main.bicepparam\n</code></pre></p> </li> </ol> <p>After a couple of minutes everything should be up and running.</p>"},{"location":"infra.html#design","title":"Design","text":"<p>The high level diagram of the infrastructure can be seen in the following image.</p> <p></p>"},{"location":"infra.html#networking-and-security","title":"Networking and Security","text":"<p>When it comes to networking, the important points are:</p> <ul> <li>There are two VNets, <code>oper</code> and <code>dev</code>. Communication between the VNets is not permitted.</li> <li>There is a Keyvault for storing secrets. The Keyvault is only accessible from within the VNets.</li> <li>The <code>oper</code> VNet has two Subnets: <code>public</code> which is accessible from the internet and <code>compute</code> where the HPC cluster is running.   Direct communication between the two Subnets is not permitted.    If the VMs of the <code>public</code> subnet need data they should read them from the storage account where the VMs of the <code>compute</code> subnet wrote them to.</li> <li>The <code>dev</code> VNet only has the <code>compute</code> subnet. </li> </ul>"},{"location":"infra.html#storage","title":"Storage","text":"<p>As we can see there are 3 Storage Accounts: </p> <ul> <li><code>pub-sa</code> where the operational cluster stores the data that are meant to be disseminated to 3rd parties</li> <li><code>oper-sa</code> where the operational cluster stores that data that are needed for its operation</li> <li><code>dev-sa</code> where the development cluster stores that data that are needed for its operation</li> </ul> <p>It should be noted that:</p> <ul> <li>The operational cluster does not have access to <code>dev-sa</code></li> <li>The development cluster has RO only access to <code>pub-sa</code> and <code>dev-sa</code>.    The idea is that if there is any issue with the model output, the developers of the model should be able to access the data    in order to debug it. The same is true for input data. You probably only want to store them once in <code>oper-sa</code></li> </ul>"},{"location":"infra.html#public-subnet","title":"Public Subnet","text":"<p>In the current implementation the <code>Public SNet</code>  is very simple. It has just a single VM which is being used to read data from  <code>pub-sa</code> and visualize them via a web-app.</p> <p>Note</p> <p>The actual deployment of the web application is omitted since it is implementation specific.</p> <p></p>"},{"location":"infra.html#compute-subnet","title":"Compute Subnet","text":""},{"location":"infra.html#naming-conventions","title":"Naming conventions","text":"<p>This naming convention is intended to bring uniformity and predictability to resource deployment and management. It supports efficient searching, sorting, and automation processes,  making it easier for users and systems to navigate our Azure infrastructure.</p> <p>This section explains the structure of the naming convention and the rationale behind it  to help users understand how to locate and identify Azure resources within the system.</p>"},{"location":"infra.html#naming-structure","title":"Naming Structure","text":"<p>The general pattern for naming Azure resources is as follows:</p> <pre><code>{project}-{environment}-{domain}-[{identifier}-]{resource_suffix}\n</code></pre> <p>Each segment of the naming pattern plays a specific role:</p> <ul> <li> <p><code>{project}</code>: This is the project identifier, a shorthand for the project or application to which the resource belongs.   Please keep it short (3-5 chars) since Azure has relatively short limits for Storage Accounts names.</p> </li> <li> <p><code>{environment}</code>: Resources are categorized by the environment they reside in.   The environments we make use of are <code>dev</code> for development and <code>oper</code> for operational or production.   There is also the <code>common</code> environment which contains resources that are not environment-specific   (e.g. Keyvault and Storage accounts).</p> </li> <li> <p><code>{domain}</code>: This denotes the functional area or service that the resource is a part of,   such as <code>net</code> for network-related resources, <code>storage</code> for storage-related resources etc.</p> </li> <li> <p><code>{identifier}</code>: Used when there are multiple similar resources that need differentiation,    such as two subnets or network security groups within the same <code>domain</code>.    For example, <code>public</code> and <code>compute</code> are identifiers that help distinguish between different subnets.</p> </li> <li> <p><code>{resource_suffix}</code>: This is a short code that represents the type of Azure resource.</p> </li> </ul>"},{"location":"infra.html#special-considerations","title":"Special Considerations","text":"<p>Due to Azure requirements, Storage Account names cannot contain hyphens and must be globally unique.  With the naming pattern we use you shouldn't run into duplicates, but in the rare chance that you do try to use a different value in <code>project</code>.</p>"},{"location":"infra.html#list-of-suffixes","title":"List of suffixes","text":"<p>The suffixes that we make use of are:</p> <ul> <li><code>rg</code> for Resource Groups</li> <li><code>vm</code> for Virtual Machines</li> <li><code>vmss</code> for Virtual Machine Scale Sets</li> <li><code>vnet</code> for Virtual Networks</li> <li><code>snet</code> for Subnets</li> <li><code>nsg</code> for Network Security Groups</li> <li><code>kv</code> for Keyvaults</li> <li><code>sa</code> for Storage Accounts</li> <li><code>nic</code> for Network Interfaces</li> <li><code>pip</code> for Public IP Addresses</li> </ul>"},{"location":"infra.html#resources-and-resource-groups","title":"Resources and Resource groups","text":""},{"location":"infra.html#list-of-resourcegroups","title":"List of <code>ResourceGroups</code>","text":"<p>After deployment you should end up with the following Resource Groups:</p> <pre><code>$ PROJECT=aaa\n$ az group list --query \"[?starts_with(name, '${PROJECT}')].name\" --output table\naaa-common-keyvault-rg\naaa-common-storage-dev-rg\naaa-common-storage-oper-rg\naaa-common-storage-pub-rg\naaa-dev-compute-rg\naaa-dev-network-rg\naaa-oper-compute-rg\naaa-oper-network-rg\naaa-oper-visual-rg\n</code></pre>"},{"location":"infra.html#list-of-resources","title":"List of <code>Resources</code>","text":"<p>After deployment you should end up with the following Resources:</p> <pre><code>$ PROJECT=aaa\n$ az resource list --query \"[?starts_with(name, '${PROJECT}')].name\" --output tsv | sort\n</code></pre>"},{"location":"provisioning.html","title":"Provisioning","text":""},{"location":"provisioning.html#dependency-installation","title":"Dependency installation","text":""},{"location":"provisioning.html#authentication","title":"Authentication","text":""},{"location":"provisioning.html#playbook-execution","title":"Playbook execution","text":""},{"location":"scratch.html","title":"Scratch","text":""},{"location":"scratch.html#deployed-resources","title":"Deployed Resources","text":"<p>Let's assume that the chosen <code>project</code> is going to be <code>ppw</code>. After deploying we should have the following resource groups:</p> <p><pre><code>- ppw-oper-control-rg\n</code></pre> In that case all the resources that are going to be deployed are:</p> <pre><code>## OPERATIONAL\n# Compute\n- ppw-oper-control-rg\n- ppw-oper-control-vm\n- ppw-oper-cluster-rg\n- ppw-oper-cluster-vmss\n# Visualization\n- ppw-oper-visual-rg\n- ppw-oper-visual-vm\n# Networking\n- ppw-oper-net-rg\n- ppw-oper-net-vnet\n- ppw-oper-net-public-snet\n- ppw-oper-net-public-nsg\n- ppw-oper-net-compute-snet\n- ppw-oper-net-compute-nsg\n\n## DEV\n# Compute + Visualization\n_ ppw-dev-jupyter-rg\n_ ppw-dev-jupyter-vm\n_ ppw-dev-jupyter-vmss\n# Networking\n_ ppw-dev-net-rg\n_ ppw-dev-net-vnet\n_ ppw-dev-net-compute-snet\n_ ppw-dev-net-compute-nsg\n\n## COMMON\n# Keyvault\n- ppw-common-keyvault-rg\n- ppw-common-keyvault-kv\n# storage\n- ppw-common-storage-main-rg\n- ppwcommonstoragemainsa\n- ppw-common-storage-oper-rg\n- ppwcommonstorageopersa\n- ppw-common-storage-dev-rg\n- ppwcommonstoragedevsa\n</code></pre> <p>ppw-oper-control-rg: A resource group for control-related resources in the operational/production environment. ppw-dev-jupyter-rg: A resource group for Jupyter-related resources in the development environment. ppw-common-keyvault-rg: A resource group for Key Vault resources shared across environments. Virtual Machines:</p> <p>ppw-oper-control-vm: A control virtual machine in the operational/production environment. ppw-dev-jupyter-vm: A Jupyter virtual machine in the development environment. Virtual Networks:</p> <p>ppw-oper-net-vnet: The main virtual network for the operational/production environment. ppw-dev-net-vnet: The main virtual network for the development environment. Subnets and Security Groups:</p> <p>ppw-oper-net-public-snet: A subnet for public access within the operational/production environment's network. ppw-oper-net-compute-nsg: A network security group for compute resources within the operational/production environment's network.</p>"}]}